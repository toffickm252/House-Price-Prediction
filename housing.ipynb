{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5060c8ff",
   "metadata": {},
   "source": [
    "## Housing Price Predictor \n",
    "#### Learning by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fcb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "data = pd.read_csv('C:\\\\Users\\\\Surface\\\\OneDrive\\\\Documentos\\\\GitHub\\\\House-Price-Prediction\\\\data\\\\Housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d626bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Exploration\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65defbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dee74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['furnishingstatus'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['mainroad'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding Binary Columns\n",
    "# List the 6 columns that are expected to be 'yes'/'no'\n",
    "binary_cols = ['mainroad', 'guestroom', 'basement', \n",
    "               'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "\n",
    "# Apply the mapping using the Pandas .replace() method\n",
    "data[binary_cols] = data[binary_cols].replace({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7768db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['mainroad'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding the Multiclass Column\n",
    "# Apply One-Hot Encoding to the 'furnishingstatus' column\n",
    "# drop_first=True is used to avoid multicollinearity\n",
    "df = pd.get_dummies(data, columns=['furnishingstatus'], drop_first=True, dtype=int)\n",
    "data = df.copy()\n",
    "# Check the new shape and the new column names\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data for the Mode\n",
    "# Create the target variable (y) - the variable we want to predict\n",
    "y = df['price']\n",
    "\n",
    "# Create the feature matrix (X) by dropping the target column\n",
    "# axis=1 specifies that we are dropping a column\n",
    "X = df.drop('price', axis=1)\n",
    "\n",
    "# Check the shapes to confirm\n",
    "print(\"Shape of X (Features):\", X.shape)\n",
    "print(\"Shape of y (Target):\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Note: random_state=42 ensures the split is the same every time you run the code.\n",
    "\n",
    "# Check the shapes to confirm the split\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ba728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSERT THIS CODE AFTER train_test_split AND BEFORE StandardScaler ---\n",
    "\n",
    "# 1. Calculate the necessary statistical quartiles\n",
    "Q1 = y_train.quantile(0.25)\n",
    "Q3 = y_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper boundary (1.5 * IQR is the standard outlier definition)\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# 2. Filter the training data to remove prices above the limit\n",
    "# We create a new, filtered y_train\n",
    "y_train_filtered = y_train[y_train < upper_limit]\n",
    "\n",
    "# We must use the exact same index from the filtered target variable \n",
    "# to filter the corresponding rows from the feature matrix (X_train)\n",
    "X_train_filtered = X_train.loc[y_train_filtered.index]\n",
    "\n",
    "# Finally, update the variables to the new, filtered data\n",
    "y_train = y_train_filtered\n",
    "X_train = X_train_filtered\n",
    "\n",
    "print(\"Outliers removed from training set.\")\n",
    "print(f\"New X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling: Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 1. Fit the scaler ONLY on the training data (X_train) and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 2. Transform the test data using the fitted scaler (DO NOT re-fit)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new log-transformed target variables\n",
    "y_train= np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "print(\"Target variables successfully log-transformed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "# Initialize the Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train the model using the scaled training data (X_train_scaled) \n",
    "# and the target prices (y_train)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Model Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1208a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction\n",
    "# Generate predictions for the unseen test data\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "print(\"Predictions generated and stored in 'y_pred'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b587ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate the R-squared score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"R-squared Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a16e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np \n",
    "\n",
    "# 1. Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# 2. Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
